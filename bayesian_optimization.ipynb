{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ml packages\n",
    "\n",
    "import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import sklearn.model_selection as model_selection\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import (CountVectorizer, HashingVectorizer, TfidfVectorizer)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "\n",
    "from hyperopt import hp, tpe, fmin, Trials, STATUS_OK\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import scale, normalize\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a3dee568776c08512c89</td>\n",
       "      <td>What is the role of Lua in Civ4?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bdb84f519e7b46e7b7bb</td>\n",
       "      <td>What are important chapters in Kannada for 10 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29c88db470e2eb5c97ad</td>\n",
       "      <td>Do musicians get royalties from YouTube?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3387d99bf2c3227ae8f1</td>\n",
       "      <td>What is the difference between Scaling Social ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>e79fa5038f765d0f2e7e</td>\n",
       "      <td>Why do elevators go super slow right before th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  a3dee568776c08512c89                   What is the role of Lua in Civ4?   \n",
       "1  bdb84f519e7b46e7b7bb  What are important chapters in Kannada for 10 ...   \n",
       "2  29c88db470e2eb5c97ad           Do musicians get royalties from YouTube?   \n",
       "3  3387d99bf2c3227ae8f1  What is the difference between Scaling Social ...   \n",
       "4  e79fa5038f765d0f2e7e  Why do elevators go super slow right before th...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dataset\n",
    "train_df = pd.read_csv(\"../AskReddit Dataset/AskReddit Dataset/train.csv\")\n",
    "test_df = pd.read_csv(\"../AskReddit Dataset/AskReddit Dataset/test.csv\")\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16                                                     What stupid things do Indians do when in your country?\n",
       "31                             Can I sue my parents for giving birth to me when I did not want them to do so?\n",
       "32                          What are your views about sexual relationship between a widow mother and her son?\n",
       "33        You became an atheist, and after 2 years you fall and break your back. You are left paralyzed fr...\n",
       "90                                    Why aren't we protesting for government control instead of gun control?\n",
       "                                                         ...                                                 \n",
       "652967              What is a liberal's understanding of the difference between pollution and climate change?\n",
       "653021    Do unattractive or average-looking men ever get a girlfriend who actually loves them or do they ...\n",
       "653029                                                                   How can I grab my aunties boobs! :p?\n",
       "653034                                                            Any girls like to be treated like sex toys?\n",
       "653049    I'm liberal, but also concerned that a lot of Quora questions are phrased to make conservatives ...\n",
       "Name: question_text, Length: 40405, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# choose elements from df where target = 1\n",
    "pd.set_option(\"display.max_colwidth\", 100)\n",
    "df_1 = train_df[train_df[\"target\"] == 1]\n",
    "df_1[\"question_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    612656\n",
       "1     40405\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# see value count order of target\n",
    "train_df[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.93813\n",
       "1    0.06187\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# percentage of troll questions in the dataset\n",
    "train_df[\"target\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a preprocessing class\n",
    "class Preprocessor:\n",
    "    def __init__(self, df) -> None:\n",
    "        self.df = df\n",
    "\n",
    "    # convert all charecters to lower case\n",
    "    def convertToLower(self):\n",
    "        self.df[\"question_text\"] = self.df[\"question_text\"].apply(lambda x: x.lower())\n",
    "        return self.df\n",
    "\n",
    "    # remove stop words\n",
    "    def removeStopWords(self):\n",
    "        stop = stopwords.words(\"english\")\n",
    "        self.df[\"question_text\"] = self.df[\"question_text\"].apply(\n",
    "            lambda x: \" \".join([word for word in x.split() if word not in stop])\n",
    "        )\n",
    "        return self.df\n",
    "\n",
    "    # remove punctuation\n",
    "    def removePunctuation(self):\n",
    "        self.df[\"question_text\"] = self.df[\"question_text\"].str.replace(\"[^\\w\\s]\", \"\")\n",
    "        return self.df\n",
    "\n",
    "    # remove numbers\n",
    "    def removeNumbers(self):\n",
    "        self.df[\"question_text\"] = self.df[\"question_text\"].str.replace(\"[0-9]\", \"\")\n",
    "        return self.df\n",
    "\n",
    "    # remove whitespaces\n",
    "    def removeWhitespaces(self):\n",
    "        self.df[\"question_text\"] = self.df[\"question_text\"].apply(\n",
    "            lambda x: \" \".join(x.split())\n",
    "        )\n",
    "        return self.df\n",
    "\n",
    "    # remove urls\n",
    "    def removeURLs(self):\n",
    "        self.df[\"question_text\"] = self.df[\"question_text\"].str.replace(\n",
    "            \"https?://\\S+|www\\.\\S+\", \"\"\n",
    "        )\n",
    "        return self.df\n",
    "\n",
    "    # snowball stemmer algorithm\n",
    "    def snowballstemmer(self):\n",
    "        stemmer = SnowballStemmer()\n",
    "\n",
    "        def stem_words(text):\n",
    "            return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "        self.df[\"question_text\"] = self.df[\"question_text\"].apply(\n",
    "            lambda x: stem_words(x)\n",
    "        )\n",
    "        return self.df\n",
    "\n",
    "    # port stemmer algorithm\n",
    "    def porterstemmer(self):\n",
    "        stemmer = PorterStemmer()\n",
    "\n",
    "        def stem_words(text):\n",
    "            return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "\n",
    "        self.df[\"question_text\"] = self.df[\"question_text\"].apply(\n",
    "            lambda x: stem_words(x)\n",
    "        )\n",
    "        return self.df\n",
    "\n",
    "    # lemmatizing\n",
    "    def lemmatize(self):\n",
    "        from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        def lemmatize_words(text):\n",
    "            return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "        self.df[\"question_text\"] = self.df[\"question_text\"].apply(\n",
    "            lambda x: lemmatize_words(x)\n",
    "        )\n",
    "        return self.df\n",
    "\n",
    "    # remove id and index columns\n",
    "    def removeUnwantedCols(self, col):\n",
    "        print(self.df.shape)\n",
    "        self.df = self.df.drop(col, axis=1)\n",
    "        return self.df\n",
    "\n",
    "    # word tokenization using nltk\n",
    "    def wordTokenization(self):\n",
    "        self.df[\"question_text\"] = self.df[\"question_text\"].apply(\n",
    "            lambda x: nltk.word_tokenize(x)\n",
    "        )\n",
    "        return self.df\n",
    "        \n",
    "\n",
    "    def preprocess(self):\n",
    "        self.df = self.convertToLower()\n",
    "        # self.df = self.removeStopWords()\n",
    "        # self.df = self.removePunctuation()\n",
    "        # self.df = self.removeNumbers()\n",
    "        # self.df = self.removeURLs()\n",
    "        # self.df = self.removeWhitespaces()\n",
    "        # self.df = self.snowballstemmer()\n",
    "        # self.df = self.porterstemmer()\n",
    "        # self.df = self.lemmatize()\n",
    "        # self.df = self.wordTokenization()\n",
    "        self.df = self.removeUnwantedCols([\"qid\"])\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(653061, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what is the role of lua in civ4?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>what are important chapters in kannada for 10 icse 2018?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>do musicians get royalties from youtube?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>what is the difference between scaling social enterprises and social franchising?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>why do elevators go super slow right before the doors open?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                       question_text  \\\n",
       "0                                                   what is the role of lua in civ4?   \n",
       "1                           what are important chapters in kannada for 10 icse 2018?   \n",
       "2                                           do musicians get royalties from youtube?   \n",
       "3  what is the difference between scaling social enterprises and social franchising?   \n",
       "4                        why do elevators go super slow right before the doors open?   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preproccesor = Preprocessor(train_df)\n",
    "preprocessed_df = preproccesor.preprocess()\n",
    "preprocessed_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(653061, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get shape of preprocessed_df\n",
    "preprocessed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a get train and test data class\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "class TrainTestData:\n",
    "    def __init__(self, trainDf, testDf) -> None:\n",
    "        self.trainDf = trainDf\n",
    "        self.testDf = testDf\n",
    "\n",
    "    def doSmote(self):\n",
    "        sm = SMOTE()\n",
    "        self.X, self.Y = sm.fit_resample(self.X, self.Y)\n",
    "        return self.trainData, self.testData\n",
    "\n",
    "    def doDecomposition(self):\n",
    "        lsa = TruncatedSVD(n_components=2)\n",
    "        lsa.fit(self.X)\n",
    "        self.trainData = lsa.transform(self.X)\n",
    "        self.testData = lsa.transform(self.testData)\n",
    "        \n",
    "\n",
    "    def get_X(self, minDocumentCount):\n",
    "\n",
    "        # concatinate trainDf and testDf\n",
    "        # self.resampling()\n",
    "        self.appendDf = pd.concat(\n",
    "            [self.trainDf[\"question_text\"], self.testDf[\"question_text\"]], axis=0\n",
    "        )\n",
    "\n",
    "        token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "        vectorizer = CountVectorizer()\n",
    "        #vectorizer = TfidfVectorizer(min_df=5,ngram_range=(1,3),tokenizer=token.tokenize)\n",
    "        # lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize\n",
    "        vectorizer.fit(self.appendDf)\n",
    "\n",
    "        self.trainData = vectorizer.transform(self.trainDf[\"question_text\"])\n",
    "        print(self.trainData.shape)\n",
    "\n",
    "        self.testData = vectorizer.transform(self.testDf[\"question_text\"])\n",
    "        print(self.testData.shape)\n",
    "        self.X = self.trainData\n",
    "\n",
    "        # self.doDecomposition() \n",
    "        return self.X\n",
    "\n",
    "    def resampling(self):\n",
    "        from sklearn.utils import resample\n",
    "        zero_data = self.trainDf[self.trainDf['target'] == 0]\n",
    "        one_data = self.trainDf[self.trainDf['target'] == 1]\n",
    "        self.trainDf = pd.concat([resample(zero_data, replace=True, n_samples=len(one_data)*6), one_data])\n",
    "        return self.trainDf\n",
    "\n",
    "    def get_Y(self):\n",
    "        # self.resampling()\n",
    "        self.Y = self.trainDf[\"target\"]\n",
    "        return self.Y\n",
    "\n",
    "    def testTrainSplit(self):\n",
    "        # self.doSmote()\n",
    "        (\n",
    "            self.X_train,\n",
    "            self.X_test,\n",
    "            self.Y_train,\n",
    "            self.Y_test,\n",
    "        ) = model_selection.train_test_split(\n",
    "            self.X, self.Y, test_size=0.2, random_state=0\n",
    "        )\n",
    "        return self.X_train, self.X_test, self.Y_train, self.Y_test\n",
    "\n",
    "    def get_X_test(self):\n",
    "        return self.testData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(653061, 2)\n",
      "(653061, 195000)\n",
      "(653061, 195000)\n"
     ]
    }
   ],
   "source": [
    "testPreprocessor = Preprocessor(test_df)\n",
    "preprocessed_test_df = testPreprocessor.preprocess()\n",
    "preprocessed_test_df.head()\n",
    "\n",
    "getTTData = TrainTestData(preprocessed_df, preprocessed_test_df)\n",
    "X = getTTData.get_X(1)\n",
    "y = getTTData.get_Y()\n",
    "X_train, X_test, Y_train, Y_test = getTTData.testTrainSplit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "'logistic_regression' : LogisticRegression,\n",
    "'knn' : KNeighborsClassifier,\n",
    "'svc' : SVC}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_space(model):\n",
    "    model = model.lower()\n",
    "    space = {}\n",
    "    if model == 'knn':\n",
    "        space = {\n",
    "            'n_neighbors': hp.choice('n_neighbors', range(1,100)),\n",
    "            'scale': hp.choice('scale', [0, 1]),\n",
    "            'normalize': hp.choice('normalize', [0, 1]),\n",
    "            }\n",
    "    elif model == 'svc':\n",
    "        space = {\n",
    "            'C': hp.uniform('C', 0, 20),\n",
    "            'kernel': hp.choice('kernel', ['linear', 'sigmoid', 'poly', 'rbf']),\n",
    "            'gamma': hp.uniform('gamma', 0, 20),\n",
    "            'scale': hp.choice('scale', [0, 1]),\n",
    "            'normalize': hp.choice('normalize', [0, 1]),\n",
    "            }\n",
    "    elif model == 'logistic_regression':\n",
    "        space = {\n",
    "            'warm_start' : hp.choice('warm_start', [True, False]),\n",
    "            'fit_intercept' : hp.choice('fit_intercept', [True, False]),\n",
    "            'tol' : hp.uniform('tol', 0.00001, 0.0001),\n",
    "            'C' : hp.uniform('C', 0.05, 3),\n",
    "            'solver' : hp.choice('solver', ['newton-cg', 'lbfgs', 'liblinear']),\n",
    "            'max_iter' : hp.choice('max_iter', range(100,1000)),\n",
    "            'scale': hp.choice('scale', [0, 1]),\n",
    "            'normalize': hp.choice('normalize', [0, 1]),\n",
    "            'multi_class' : 'auto',\n",
    "            'class_weight' : 'balanced'\n",
    "                }\n",
    "    space['model'] = model\n",
    "    return space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc_status(clf,X_,y):\n",
    "    acc = cross_val_score(clf, X_, y, cv=5).mean()\n",
    "    return {'loss': -acc, 'status': STATUS_OK}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obj_fnc(params):\n",
    "    model = params.get('model').lower()\n",
    "    X_ = scale_normalize(params,X[:])\n",
    "    del params['model']\n",
    "    clf = models[model](**params)\n",
    "    return(get_acc_status(clf,X_,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_11652/3003882676.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mhypopt_trials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m best_params = fmin(obj_fnc, search_space(model), algo=tpe.suggest,\n\u001b[0m\u001b[0;32m      3\u001b[0m max_evals=1000, trials= hypopt_trials)\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhypopt_trials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'result'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'loss'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "hypopt_trials = Trials()\n",
    "best_params = fmin(obj_fnc, search_space(model), algo=tpe.suggest,\n",
    "max_evals=1000, trials= hypopt_trials)\n",
    "print(best_params)\n",
    "print(hypopt_trials.best_trial['result']['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
